---
title: "Homework_3"
output: github_document
---
```{r}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggridges)
library(patchwork)
library(rnoaa)
install.packages("hexbin", repos ="http://cran.us.r-project.org")
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1
```{r}
# install.packages("devtools")
devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)
data("instacart")  
```

This dataset contains `r nrow(instacart)` rows and ...columns.

Observations are the level of items in orders by user. There are some user/order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes.

How many aisles, and which are most items from?

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))

```

let's make a plot

```{r}
instacart %>% 
   count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust = 1))
```


Let's make a table!

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```


Apples vs ice cream

```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```



## Problem 2

Import and tidy data
```{r import and tidy accel_data}
accel_data = 
  read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate_at(vars(activity_1:activity_1400), as.numeric) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute_of_day",
    values_to = "activity_count",
    names_prefix = "activity") %>% 
 mutate(weekday_end = case_when(
day %in% c("Monday","Tuesday","Wednesday","Thursday","Friday") ~ "weekday",
day %in% c("Saturday","Sunday") ~ "weekend")) %>% 
  relocate("week", "weekday_end", "day", "day_id", "minute_of_day", "activity_count") %>% 
   mutate_at(vars("weekday_end","day"), as.factor) %>% 
  mutate_at(vars("day_id", "week", "activity_count"), as.double) %>% 
  mutate(
  day = forcats::fct_relevel(day, "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")
  )
  
  
```

This data was collected over 5 weeks from a 63 year-old male diagnosed with CHF. Variables in this dataset include week, weekday/end (here,coded as weekday_end), day (Sunday - Saturday), day of the month (day_id), and activity count. This dataset contains `r nrow(accel_data)` observations, which were collected at each minute of the day, every day, for 5 weeks.



group by and summarize into table
```{r accel_data table}
agg_accel = 
accel_data %>% 
  group_by(day_id, day, week) %>% 
  summarize(activity = sum(activity_count)) %>% 
  

knitr::kable()

agg_accel
```
Fridays tend to be a high activity day, while weekends seem to have lower activity. Saturdays especially have a lower activity count. Weeks 1, 2, and 5 seem to have a high activity count, with the exception of Saturday in Week 5. Week 4 seems to have a lower relative activity count as compared to other weeks of the same month. An activity count of 1440 is repeated twice - this number may have some significance.  



Single-panel plot
```{r single panel plot of accel_data}
accel_data %>%
  ggplot(aes(x = minute_of_day, y = activity_count)) +
  geom_line(aes(), alpha = .3) +
  labs(
    title = "Activity Plotted Over 5-Week Period",
    x = "Minute of Day",
    y = "Activity Count",
    caption = "data from accelerometer dataset"
  ) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

# I was unable to figure out what is going on with my x-axis, and any code I created to adjust it resulted in my document being un
```


## Problem 3

Load and tidy data
```{r load and tidy nyc_noaa data}
library(p8105.datasets)
data("ny_noaa")

skimr::skim(ny_noaa)

```
This dataset from the National Oceanic and Atmospheric Association describes the weather in New York from all NY weather stations between January 1st, 1981 to December 31st, 2010. The data contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. Variables include station ID, date of data collection, and measurements of precipitation, snow, snow depth, and maximum + minimum temperatures per day. There is significant missing data, especially as related to minimum and maximum temperatures - both tmin and tmax have over 1 million missing values.



Clean data and create separate variables for year, month, day; adjust variables to proper units and find mode
```{r clean data, separate vars, and adjust vars, find mode}
tidy_noaa = (
  ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), convert = "TRUE") %>% 
  mutate(
  tmin = as.double(tmin),
  tmax = as.double(tmax)
) %>% 
  mutate(
  (tmin = (tmin/10)),
  (tmax = (tmax/10)),
  (prcp = (prcp/10))
  ))

skimr::skim(tidy_noaa)

  tidy_noaa %>% 
  count(snow) %>% 
  arrange(snow)

tidy_noaa
```
The most commonly observed value in this dataset is a snowfall amount of 0mm. This is likely because snow typically only falls within a narrow timeframe of the year (select days in winter). 



Problem 3, part 2
```{r 2-panel plot}
  tidy_noaa %>% 
  mutate(month = as.character(month)) %>% 
  filter(month == c(1,7)) %>%
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax)) %>% 
  ggplot(aes(x = year, y = mean_tmax)) +
  geom_point(alpha = 0.5) +
  geom_smooth(se = FALSE) +
 facet_grid(. ~month) +
  labs(
    title = "Avg Max Temps in January and July from 1981 to 2010",
    x = "year",
    y = "mean maximum temperature",
    caption = "data from New York NOAA"
  )

# I was unable to troubleshoot the temperature issue. I recognize that there is no place inhabitable by humans at 300 degrees celsius (that's a bit crispy), and I don't think that even Siberia gets to -100 degrees celsius. I apologize. 
```
```
Observations include: July has higher maximum temperatures, on average, than January. Temperatures seem to fluctuate in cycles. 1994, 2003, and 2004 appear to have been a particularly cold winters, though it is difficult to determine whether they are outliers. January temperatures seem to fluctuate more than those in July. July 1987 appears to be an outlier in that it was much colder than average July temperatures. 


tmax vs. tmin
```{r 3i}
tidy_noaa %>% 
ggplot(aes(x = tmin, y = tmax)) +
geom_hex() +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

```


snowfall distribution
```{r 3ii}
tidy_noaa %>% 
filter(
snow > 0, 
snow < 100
) %>% 
group_by(year) %>% 
ggplot(aes(x = snow, y = snow)) +
geom_boxplot() + 
facet_wrap(.~ year)
```



